{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook for playing around with cuda, first using libs like pytorch and keops, and then using real cuda kernel code with pyCUDA."
      ],
      "metadata": {
        "id": "MwbVx0_0pSrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nearest neighbor with cuda"
      ],
      "metadata": {
        "id": "_I6iv_nwMjsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea : test the K-nn algorithm optimization tricks in CUDA introduced in the [7th MVA class of Jean Feydy](https://www.jeanfeydy.com/Teaching/index.html)\n",
        "\n",
        "x_i, y_i are arrays ;\n",
        "We want the nearest neighbor in y_i for each term of x_i."
      ],
      "metadata": {
        "id": "n4TH9PgvK05Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1/ Pytorch implementation (cpu)"
      ],
      "metadata": {
        "id": "WDbjDP5-LIcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "n = 5000\n",
        "m = 2500\n",
        "d = 100\n",
        "\n",
        "#take yi = xi for nn inside the same group\n",
        "xi = torch.rand((n,d))\n",
        "yi = torch.rand((m,d))\n",
        "\n",
        "\n",
        "def nn_cpu(xi, yi):\n",
        "  deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
        "  distances = torch.sum(deltas**2, dim=2)\n",
        "  return distances.argmin(dim=0)\n"
      ],
      "metadata": {
        "id": "URI909pOIkKj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "iXivyn7sV_jd"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strange behavior from timeit command, we will use the torch timer anyways, which is adapted to cuda code."
      ],
      "metadata": {
        "id": "30mH0udhTv_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "t0 = benchmark.Timer(\n",
        "    stmt='nn_cpu(xi, yi)',\n",
        "    setup=\"\"\"\n",
        "    def nn_cpu(xi, yi):\n",
        "      deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
        "      distances = torch.sum(deltas**2, dim=2)\n",
        "      return distances.argmin(dim=0)\"\"\",\n",
        "    globals={'xi': xi,\"yi\":yi,\"n\":n,\"m\":m,\"d\":d})\n",
        "\n",
        "print(t0.timeit(1))\n",
        "#too slow !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnQWTxGBRKhM",
        "outputId": "9dec61c4-ddae-4934-c477-9fcc599bc67b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7e7c8d4d90f0>\n",
            "nn_cpu(xi, yi)\n",
            "setup:\n",
            "  def nn_cpu(xi, yi):\n",
            "    deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
            "    distances = torch.sum(deltas**2, dim=2)\n",
            "    return distances.argmin(dim=0)\n",
            "\n",
            "  8.37 s\n",
            "  1 measurement, 1 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2/Pytorch implementation (gpu)"
      ],
      "metadata": {
        "id": "cfZl9xylNkMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "#take yi = xi for nn inside the same group\n",
        "xi = torch.rand((n,d)).to(device)\n",
        "yi = torch.rand((m,d)).to(device)\n",
        "\n",
        "def nn_gpu(xi,yi):\n",
        "  deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
        "  distances = torch.sum(deltas**2, dim=2)\n",
        "  return distances.argmin(dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "fRUyL6AMMiRE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = benchmark.Timer(\n",
        "    stmt='nn_gpu(xi, yi)',\n",
        "    setup=\"\"\"\n",
        "    xi = torch.rand((n,d)).cuda()\n",
        "    yi = torch.rand((m,d)).cuda()\n",
        "\n",
        "    def nn_gpu(xi,yi):\n",
        "      deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
        "      distances = torch.sum(deltas**2, dim=2)\n",
        "      return distances.argmin(dim=0)\n",
        "\"\"\",\n",
        "    globals={\"n\":n,\"m\":m,\"d\":d})\n",
        "\n",
        "print(t1.timeit(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCimitQWT64N",
        "outputId": "4d604166-70f7-458a-8d5e-0886fcc90ab9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7d87077e50c0>\n",
            "nn_gpu(xi, yi)\n",
            "setup:\n",
            "  xi = torch.rand((n,d)).cuda()\n",
            "  yi = torch.rand((m,d)).cuda()\n",
            "\n",
            "  def nn_gpu(xi,yi):\n",
            "    deltas = xi.view((1,n,d)) - yi.view((m,1,d))\n",
            "    distances = torch.sum(deltas**2, dim=2)\n",
            "    return distances.argmin(dim=0)\n",
            "\n",
            "  107.02 ms\n",
            "  1 measurement, 10 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better"
      ],
      "metadata": {
        "id": "k3y4hMXdaMII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch implementation optimized\n",
        "\n",
        "Using the fact that $\\|x_k - y_j \\|^2 = x_k^2 +y_j^2 - 2 x_k y_j$\n",
        "\n",
        "Thus, $\\text{argmin}_j \\|x_k - y_j \\|^2 = \\text{argmin}_j -2x_k y_j +y_j^2$\n",
        "\n",
        "We can pre-compute the y_j to save memory inside the GPU, in order to speed up computation and avoid unnecessary memory transfers between the GPU's global memory and shared memory\n"
      ],
      "metadata": {
        "id": "xTXHn1jUNwGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "#take yi = xi for nn inside the same group\n",
        "xi = torch.rand((n,d)).to(device)\n",
        "yi = torch.rand((m,d)).to(device)\n",
        "\n",
        "def nn_gpu_opti(xi,yi):\n",
        "  y_sq = torch.sum(yi**2, dim=1)\n",
        "  dot = -2 * xi@yi.T\n",
        "  dist_minus_x_sq = y_sq.view((1,m)) + dot\n",
        "  nn = dist_minus_x_sq.argmin(dim=0)\n",
        "  return nn"
      ],
      "metadata": {
        "id": "tWajJezrN3nq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t3 = benchmark.Timer(\n",
        "    stmt='nn_gpu_opti(xi, yi)',\n",
        "    setup=\"\"\"\n",
        "    xi = torch.rand((n,d)).cuda()\n",
        "    yi = torch.rand((m,d)).cuda()\n",
        "\n",
        "    def nn_gpu_opti(xi,yi):\n",
        "      y_sq = torch.sum(yi**2, dim=1)\n",
        "      dot = -2 * xi@yi.T\n",
        "      dist_minus_x_sq = y_sq.view((1,m)) + dot\n",
        "      nn = dist_minus_x_sq.argmin(dim=0)\n",
        "      return nn\n",
        "\"\"\",\n",
        "    globals={\"n\":n,\"m\":m,\"d\":d})\n",
        "\n",
        "print(t3.timeit(100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uobWnB1XP_O1",
        "outputId": "1edced69-d5cd-4389-e6d7-8d65fcce2776"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7d87077e6c20>\n",
            "nn_gpu_opti(xi, yi)\n",
            "setup:\n",
            "  xi = torch.rand((n,d)).cuda()\n",
            "  yi = torch.rand((m,d)).cuda()\n",
            "\n",
            "  def nn_gpu_opti(xi,yi):\n",
            "    y_sq = torch.sum(yi**2, dim=1)\n",
            "    dot = -2 * xi@yi.T\n",
            "    dist_minus_x_sq = y_sq.view((1,m)) + dot\n",
            "    nn = dist_minus_x_sq.argmin(dim=0)\n",
            "    return nn\n",
            "\n",
            "  1.92 ms\n",
            "  1 measurement, 100 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is A LOT faster than the previous one, we are nearly at the hardware limit because for d >= 100 with this algorithm, the memory transfer operations (inside the GPU from the global memory to the threads) are no longer the bottleneck (since one memory transfer ~100 GPU operations).\n",
        "Now let us try without even storing the distances, using the KeOos library.\n"
      ],
      "metadata": {
        "id": "-dxkqEQOaeji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using KeOps"
      ],
      "metadata": {
        "id": "6QRiV2xDWhmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykeops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ7ZWGC_Vgyz",
        "outputId": "0794600f-e667-4427-90fc-92190ff17c15"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykeops\n",
            "  Downloading pykeops-2.2.3.tar.gz (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pykeops) (1.26.4)\n",
            "Collecting pybind11 (from pykeops)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting keopscore==2.2.3 (from pykeops)\n",
            "  Downloading keopscore-2.2.3.tar.gz (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pykeops, keopscore\n",
            "  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykeops: filename=pykeops-2.2.3-py3-none-any.whl size=118636 sha256=7c0711afdd9a68c092c42ad3f4ff687b43a7bc5da73609693b1cc518343200a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/8b/c7/25e5194a7138fd564c3ef3e275ae0155c207cd85d7ab347817\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keopscore: filename=keopscore-2.2.3-py3-none-any.whl size=172483 sha256=9c5a14c199dde5b7c3cce4a3d2eb7eee1f7549de072a954da7ca8c9b6f704f32\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/d8/ee/29900acfbbd7ee0f3b05981d3d172baad4c3b5d40cbf4c5d74\n",
            "Successfully built pykeops keopscore\n",
            "Installing collected packages: pybind11, keopscore, pykeops\n",
            "Successfully installed keopscore-2.2.3 pybind11-2.13.6 pykeops-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pykeops.torch import LazyTensor\n",
        "xi_lazy = LazyTensor(xi.view((1,n,d)))\n",
        "yi_lazy = LazyTensor(yi.view((m,1,d)))\n",
        "\n",
        "D_ij = ((xi_lazy - yi_lazy) ** 2).sum(-1)\n",
        "ind_nn = D_ij.argKmin(1, dim=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0R956YtWqzI",
        "outputId": "80ca4bb8-edce-4bfe-f404-3b057f1ebdc6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KeOps] Compiling cuda jit compiler engine ... OK\n",
            "[pyKeOps] Compiling nvrtc binder for python ... OK\n",
            "[KeOps] Generating code for ArgKMin_Reduction reduction (with parameters 0) of formula Sum((a-b)**2) with a=Var(0,100,1), b=Var(1,100,0) ... OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t4 = benchmark.Timer(\n",
        "    stmt='nn_gpu(xi_lazy, yi_lazy)',\n",
        "    setup=\"\"\"\n",
        "    def nn_gpu(xi_lazy,yi_lazy):\n",
        "      D_ij = ((xi_lazy - yi_lazy) ** 2).sum(-1)\n",
        "      ind_nn = D_ij.argKmin(1, dim=1)\n",
        "      return ind_nn\n",
        "\"\"\",\n",
        "    globals={'xi_lazy': xi_lazy,\"yi_lazy\":yi_lazy,\"n\":n,\"m\":m,\"d\":d})\n",
        "\n",
        "print(t4.timeit(1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi4_UG4UWxn7",
        "outputId": "89952b70-a9dc-403f-a52f-f7546351c92c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7e7c8d4d8a60>\n",
            "nn_gpu(xi_lazy, yi_lazy)\n",
            "setup:\n",
            "  def nn_gpu(xi_lazy,yi_lazy):\n",
            "    D_ij = ((xi_lazy - yi_lazy) ** 2).sum(-1)\n",
            "    ind_nn = D_ij.argKmin(1, dim=1)\n",
            "    return ind_nn\n",
            "\n",
            "  12.95 ms\n",
            "  1 measurement, 1 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest algorithm is the handmade, optimized one but the KeOPS lib is still impressive : 10x faster than \"naive\" pytorch"
      ],
      "metadata": {
        "id": "j11TS2xYb60X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCJm6iB7ZOHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More fun with cuda"
      ],
      "metadata": {
        "id": "4mTxJlQQcUBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9zbKqXMcWfp",
        "outputId": "b783f15a-1619-4159-a792-cd2a338c2648"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.21-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Downloading pytools-2024.1.21-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.4/92.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp310-cp310-linux_x86_64.whl size=660545 sha256=f9fd31f837d5d22e5605533f44a6e3226d61af9371a91fd7cf1841fef439e3d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/63/40/4bf006182f942d3516b71bb2ff3b57ccbdb8b2c0ee81882b6e\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.8 pycuda-2024.1.2 pytools-2024.1.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## pyCUDA demo from the documentation : elementwise multiplication\n",
        "\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "import numpy as np\n",
        "\n",
        "from pycuda.compiler import SourceModule\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void multiply_them(float *dest, float *a, float *b)\n",
        "{\n",
        "  const int i = threadIdx.x;\n",
        "  dest[i] = a[i] * b[i];\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "multiply_them = mod.get_function(\"multiply_them\")\n",
        "\n",
        "a = np.random.randn(400).astype(np.float32)\n",
        "b = np.random.randn(400).astype(np.float32)\n",
        "\n",
        "dest = np.zeros_like(a)\n",
        "multiply_them(\n",
        "        drv.Out(dest), drv.In(a), drv.In(b),\n",
        "        block=(400,1,1), grid=(1,1))\n",
        "\n",
        "print(all(dest == a*b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uYhp0VYJcYdn",
        "outputId": "4d3f3b0f-2c44-42ed-e7dd-7bb0a47e13e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test : naive matrix multiplication"
      ],
      "metadata": {
        "id": "an-thEt3rxG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# A,B square matrices of dim d\n",
        "# each core i gets an element of the matrix AB : (AB)_{i,j} = sum(A_{i,k}B_{k,j})\n",
        "# with i = d * j + k\n",
        "\n",
        "mod2 = SourceModule(\"\"\"\n",
        "__global__ void matmul_naive(float *dest, float *A, float *B,int d)\n",
        "{\n",
        "  const int i = threadIdx.x;\n",
        "  const int j = threadIdx.y;\n",
        "\n",
        "  float sum = 0;\n",
        "  for (int l = 0; l < d; l++){\n",
        "    sum += A[i * d + l] * B[l * d + j];\n",
        "  }\n",
        "\n",
        "  dest[i*d + j] = sum;\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "matmul_naive = mod2.get_function(\"matmul_naive\")\n",
        "d = 10\n",
        "A = np.random.rand(d,d)\n",
        "A = A.astype(np.float32)\n",
        "B = np.random.rand(d,d)\n",
        "B = B.astype(np.float32)\n",
        "\n",
        "dest = np.zeros((d,d)).astype(np.float32)\n",
        "\n",
        "A = np.ascontiguousarray(A)\n",
        "B = np.ascontiguousarray(B)\n",
        "dest = np.ascontiguousarray(dest)\n",
        "\n",
        "matmul_naive(\n",
        "        drv.Out(dest), drv.In(A), drv.In(B), np.int32(d),\n",
        "        block=(d,d,1), grid=(1,1,1))\n",
        "\n",
        "\n",
        "d = dest - A@B\n",
        "print(np.max(d))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PkivR4ycY7W",
        "outputId": "463ce7a4-f9a4-4951-d57d-004c34fef9f1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3841858e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe algorithm works up to some precisions issues, probably due to float conversion."
      ],
      "metadata": {
        "id": "uRsUI2ZIrmwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1o6pV9SgpRdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88r_YxqPl_j9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}